# Distributed Systems

https://pdos.csail.mit.edu/6.824/schedule.html

---

## Introduction

- why distributed
    - to achieve security via isolation
    - to tolerate faults via replication
    - to scale up throughput via parallel CPUs/mem/disk/net
- topic
    - implementation
    - performance
    - fault tolerance
    - consistency

- MapReduce
    - overview
        - åŸºæœ¬æ€è·¯ã€é€‚ç”¨åœºæ™¯
        - è§£å†³ä»€ä¹ˆé—®é¢˜ï¼Œå¦‚ä½•è§£å†³
    - advantage
        - hide painful details, scale well
        - æ¯”å…¶ä»–æ–¹æ¡ˆä¼˜åŠ¿åœ¨å“ª
    - limitations
        - å“ªäº›åœºæ™¯ä¸é€‚ç”¨ï¼Œä¸ºä»€ä¹ˆä¸é€‚ç”¨
        - æ€§èƒ½ã€æ•ˆç‡çš„ç“¶é¢ˆåœ¨å“ª
    - implementation details
        - å¦‚ä½•åº”å¯¹ limitation
    - fault tolerance, crash recovery
        - å‘ç°ã€å¤„ç†ã€æ¢å¤

---

## Infrastructure: RPC and threads

- threading challenages
    - sharing data
        - synchronization
    - coordination
    - granularity
        - coarse-grained
        - fine-grained
- sharing+locks vs channels
    - state vs communication
- RPC
    - client/server communication
- RPC failure
    - failure-handling scheme
        - best effort (retry)
        - at most once (unique ID for each request)
            - how to create unique ID
            - when to discard old RPC response (heartbeat, received next UID)
        - exactly once

---

## GFS

- system design
    - trading consistency for simplicity and performance
    - performance, fault-tolerance, consistency
- consistency
    - replicate
        - multiply machines, reads and writes go across
    - strong consistency
        - bad for performance, easy for application writers
    - weak consistency
        - good for performance, easy for scale, bad for reason about
- consistency model
    - a replicated FS behaved like a non-replicated FS
        - what happens on a single machine?
    - challenges
        - concurrency
        - machine failures
        - network partitions

- GFS
    - TODO
    - read
        - 64MB chunk
        - 3x replication
        - master server
        - worker server
    - write

---

## Primary/Backup Replication

- ideal properties
    available: still useable despite  some class of failures
    strongly consistent: looks just like a single server to clients
    transparent to clients
    transparent to server software
    efficient
- fault tolerance, replication
    - two or more servers, if one replica fails, others can continue
- question
    - what state to replicate
    - when to cut over to backup
    - does primary have to wait for backup
- approaches
    - state transfer
        - primary executes the operations, sends new state to backups
        - more simpler
    - replicated state machine
        - all replicas execute all operations
        - more efficient
            - operations are small compared to data
            - complex to get right (same start state, same operations, same order, deterministic)

- Fault-Tolerant Virtual Machines
    - two machines, primary and backup
    - primary sends all inputs to backup over logging channel
    - the backup must lag by one event (one log entry)

---

## Raft

- fault-tolerant services using replicated state machines
    - same client-visible behavior as single non-replicated server
    - available despite some number of failed servers
    - each replica server executes same commands in same order
- how to avoid split brain
    - èŠ‚ç‚¹æ•…éšœäº†ï¼Œåº”è¯¥å¿½ç•¥æ•…éšœèŠ‚ç‚¹ï¼›ç½‘ç»œæ•…éšœäº†ï¼Œåº”è¯¥ç­‰å¾…ç½‘ç»œæ¢å¤
    - æ²¡æœ‰ç­‰å¾…ç½‘ç»œæ¢å¤ï¼Œç»“æœå°±æ˜¯ split brain
    - computers cannot distinguish crashed machines vs a partitioned network
- majority vote
    - 2f+1 servers to tolerate f failures (3 servers can tolerate 1 failure)
    - must get majority (f+1) of servers ti agree to make progress
        - majority is out of all 2f+1 servers, not just out of live ones
    - å°‘æ•°æœä»å¤šæ•°ï¼Œå§‹ç»ˆä¿è¯ f+1 ä¸ªèŠ‚ç‚¹çš„çŠ¶æ€æ˜¯ä¸€è‡´çš„ï¼Œä¹Ÿå°±æ˜¯æ­£ç¡®çš„çŠ¶æ€
    - å­˜æ´»çš„èŠ‚ç‚¹å°‘äº f+1 ä¸ªï¼Œé‚£ä¹ˆæŠ•ç¥¨å§‹ç»ˆæ˜¯å¤±è´¥çš„
    - ï¼ˆç½‘ç»œçœŸçš„åäº†ï¼Œé‚£ä¹ˆè¿™å°±æ˜¯ä¸ªç‰ºç‰²å¯ç”¨æ€§æ¢å–ä¸€è‡´æ€§çš„æ–¹æ¡ˆ

- goal
    - same client-visible behavior as single non-replicated server
    - available despite minority of failed/disconnected servers

- two partition-tolerant replication schemes, Paxos and View-Stamped Replication

- Raft
    - elections, log handling, persistence, client behavior, snapshots
    - overview
        - diagram: clients, k/v layer, Raft layer
            - Raft é€‰å‡º leader
            - client å‘é€æ“ä½œåˆ° leader çš„ kv
            - Raft å°†æ“ä½œå‘é€åˆ° followersï¼Œfollowers è®°å½•åˆ° local log å¹¶è¿”å›ç»™ leader
            - leader ç­‰ majority æ“ä½œæˆåŠŸï¼Œæ‰€æœ‰ replica å¼€å§‹æ“ä½œ kv
            - leader å°† kv çš„æ“ä½œç»“æœè¿”å›ç»™ client
        - log
            - log éƒ½æ˜¯æœ‰ç¼–å·çš„
            - å¯ä»¥ç¡®å®šæ“ä½œçš„æ‰§è¡Œé¡ºåº
            - åŒæ—¶ä¿è¯æ‰€æœ‰èŠ‚ç‚¹çŠ¶æ€ä¸€è‡´
    - leader election
        - client åªå’Œ leader äº¤äº’ï¼Œå€Ÿæ­¤ä¿è¯æ“ä½œçš„æ‰§è¡Œé¡ºåº
        - terms (number)
            - æ¯ä¸ª term å¯ä»¥æœ‰ä¸€ä¸ª leaderï¼ˆä¹Ÿå¯èƒ½é€‰ä¸»å¤±è´¥æ²¡æœ‰ leader
                - leader è¦æœ‰ majority åŒæ„ï¼Œæ‰€ä»¥èƒ½ä¿è¯æœ€å¤šåªæœ‰ä¸€ä¸ª leader
            - è°å¤§è°å°±æ˜¯ leaderï¼Œè°å¤§è°å°±è¯´äº†ç®—
        - when
            - leader å‘é€ heartbeat åˆ° followers
            - å¦‚æœ followers è¿‡äº†ä¸€æ®µæ—¶é—´è¿˜æ²¡æ”¶åˆ°ï¼Œå°±ä¼šè®¤ä¸º leader æ•…éšœäº†ï¼Œå¼€å§‹æ–°çš„é€‰ä¸»ç¨‹åºï¼ˆterm å¢å¤§
            - ï¼ˆè¿™ç§ç©æ³•å¯èƒ½å¯¼è‡´ä¸å¿…è¦çš„é€‰ä¸»ï¼Œä½†æ˜¯é›†ç¾¤æ•°æ®çš„æ­£ç¡®æ€§æ˜¯æœ‰ä¿è¯çš„
            - å¯èƒ½ç”±äºå…¶ä»–é—®é¢˜é€‰ä¸»å¤±è´¥ã€‚å¤±è´¥äº†ä¼šå†æ¬¡è¿›å…¥é€‰ä¸»æµç¨‹ï¼ˆterm æœ€å¤§é‚£ä¸ªå°è¯•å‘èµ·æ–°çš„é€‰ä¸»æµç¨‹
                - majority çš„èŠ‚ç‚¹éƒ½ä¸å¯ç”¨ï¼Œé€‰ä¸»ä¼šå¤±è´¥ï¼ˆä¸ç®¡æ˜¯çœŸçš„ä¸å¯èƒ½ï¼Œè¿˜æ˜¯ç½‘ç»œæ•…éšœï¼Œåæ­£æ²¡ majority åŒæ„å°±æ— æ³•è½¬å˜ä¸º leader
                - å¤šä¸ªèŠ‚ç‚¹éƒ½è¯•å›¾å˜æˆ leaderï¼Œå¯èƒ½ç”±äºå¹¶å‘å¯¼è‡´æŠ•ç¥¨å¤±è´¥ï¼ˆè¿˜æ˜¯æ²¡è¾¾åˆ° majority åŒæ„ï¼Œä½†èŠ‚ç‚¹è¿˜èƒ½æ­£å¸¸é€šä¿¡
            - å¾—åˆ° majority æ”¯æŒçš„èŠ‚ç‚¹ï¼Œå‘å¿ƒè·³ç»™æ‰€æœ‰èŠ‚ç‚¹ï¼Œè®©å¤§å®¶çŸ¥é“é€‰ä¸»å·²ç»å®Œæˆäº†
        - candidate
            - å°è¯•è½¬æ¢ä¸º leader çš„èŠ‚ç‚¹ï¼Œè¿›å…¥ candidate çŠ¶æ€
            - majority æ”¯æŒï¼Œå°±å˜æˆ leader
            - majority æ”¯æŒäº†å…¶ä»– candidateï¼Œå°±å˜å› followers
            - é€‰ä¸»å¤±è´¥ï¼Œä¿æŒ candidate çŠ¶æ€ï¼Œç­‰ timeout åé‡æ¥
        - timeout
            - randomï¼Œé¿å…å¤šä¸ª followers åŒæ—¶å°è¯•è½¬æ¢ä¸º candidate
            - lowest random delay
            - ç½‘ç»œå¯èƒ½å„ç§é—®é¢˜ï¼Œæ‰€ä»¥ timeout è‡³å°‘è¦å¤Ÿå‡ ä¸ª heartbeat æ¥å›
            - é€‰ä¸»ä¹Ÿè¦æ—¶é—´ï¼Œæ‰€ä»¥ timeout è‡³å°‘ä¹Ÿè¦å¤Ÿæ•´ä¸ªç³»ç»Ÿå®Œæˆä¸€æ¬¡é€‰ä¸»
    - Raft log
        - replicated vs. committed entries
            - committed è¯´æ˜ majority åŒæ„ï¼Œä¸ä¼šå˜äº†
            - replicated åªæ˜¯å®Œæˆå¤åˆ¶ï¼Œå¯èƒ½ä¼šå˜
        - servers can temporarily have different log
        - client åªå’Œ leader äº¤äº’ã€‚é‚£ä¹ˆ leader å‘ç”Ÿæ›´æ¢çš„æ—¶å€™ï¼Œclient ä¼šæ€ä¹ˆæ ·
        - æ›´æ¢ leader ä¹‹åï¼Œserver çš„ log å¯èƒ½å¤„åœ¨ä¸åŒçŠ¶æ€
            - force followers adopt new leader's log
            - replicated å¯èƒ½å‡ºç°å›æ»š
            - è¦ä¿è¯ä¸èƒ½å½±å“ committed çš„çŠ¶æ€
        - Raft needs to ensure elected leader has all committed log entries
            - at least as up to date
    - persistence
        - repair soon after crash to avoid dipping below a majority
        - strategies
            - ä¿®å¤é—®é¢˜ï¼Œé‡å¯èŠ‚ç‚¹ï¼ˆæ•°æ®å¯èƒ½è¿‡æœŸï¼Œä½†å¤§éƒ¨åˆ†è¿˜åœ¨
            - å¯ç”¨æ–°èŠ‚ç‚¹ï¼ˆéœ€è¦å¤åˆ¶ä¹‹å‰çš„æ•°æ®
        - persistent state
            - logList, å¦‚æœå®•æœºä¹‹å‰å±äº majority ä¸€éƒ¨åˆ†ï¼Œé‚£ä¹ˆé‡Œé¢åŒ…å« committed çš„æ•°æ®ï¼Œè¦è®°ä¸‹ä½œä¸ºè¯æ®å•Š
            - votedFor, ä¸è®¸ç»™ä¸åŒçš„ term æŠ•èµæˆç¥¨ï¼Œæ‰€ä»¥è¦è®°ä¸‹
            - currentTerm, term åªè®¸å¢ä¸è®¸å‡ï¼Œæ‰€ä»¥è¦è®°ä¸‹ä¹‹å‰çš„
        - bottleneck for performance
            - æŒä¹…åŒ–åˆ°ç¡¬ç›˜ï¼Œè¦æ—¶é—´å•Š
    - snapshots, log compaction
        - æ¢å¤çŠ¶æ€æ—¶ï¼Œå¦‚æœé‡æ”¾å®Œæ•´æ—¥å¿—ï¼Œå¤ªæ…¢
        - log å¯èƒ½æ¯” state å¤§å¾—å¤šï¼Œæ‰€ä»¥å¯ä»¥å– state çš„å¿«ç…§ï¼Œå‡å°‘éœ€è¦é‡æ”¾çš„æ—¥å¿—æ•°é‡
        - åš state å¿«ç…§çš„æ—¶å€™ï¼Œæœª committed çš„ log è¦ä¿ç•™ï¼ˆå¯èƒ½è¢«ç®—åœ¨ majority é‡Œäº†
        - snapshot reflects only committed entries
        - service's on-disk state = service's snapshot + Raft's persistent log
    - configuration change
        - configuration = set of servers
            - å¢åˆ èŠ‚ç‚¹
        - problem
            - followers will learn new configuration at different times
            - ä¼šç»™è®¡ç®— majority é€ æˆé—®é¢˜
            - ï¼ˆä¹Ÿè¦æ±‚ majority åŒæ„å³å¯ï¼Ÿé‚£ä¹ˆèŠ‚ç‚¹å®•æœºçš„æƒ…å†µï¼Œæ°¸è¿œæ— æ³•è¾¾æˆ majority?
        - joint consensus
            - avoids any time when both old and new can choose leader independently
            - administrator asks the leader to switch to new configuration
            - leader å°†æ–°é…ç½®ä¸‹å‘åˆ° followersï¼Œæ–°æ—§é…ç½®éƒ½ committed ä¹‹åï¼Œå†åˆ‡æ¢åˆ°æ–°é…ç½®
    - performance
        - many situations don't require high performance
        - (Raft) sacrifice performance for simplicity
    - faq
        - sacrifice performance for simplicity
            - æ“ä½œè¦æŒä¹…åŒ–åˆ°ç¡¬ç›˜
            - ä¹±åºåˆ°è¾¾çš„æ¶ˆæ¯ï¼Œä¼šè¢«ä¸¢å¼ƒ
            - å¿«ç…§å¯èƒ½å¾ˆå¤§ï¼ˆåˆ›å»ºã€ä¼ è¾“éƒ½æ…¢ï¼‰
            - æ—¥å¿—è¦æ±‚æœ‰åºï¼Œå¯èƒ½å¯¼è‡´æ— æ³•åˆ©ç”¨å¤šæ ¸
        - While Paxos requires some thought to understand, it is far simpler than Raft
            - But Paxos solves a much smaller problem than Raft
        - real-world systems are derived from Paxos
            - Chubby, Spanner, Megastore, ZooKeeper/ZAB
        - real-world users of Raft
            - docker, etcd, CockroachDB, RethinkDB, TiKV
        - systems can survive and continue to operate when only a minority of the cluster is active
            - do it with different assumptions, or different client-visible semantics
            - human decideï¼Œæœºå™¨ä¸çŸ¥é“å®•æœºè¿˜æ˜¯ç½‘ç»œé—®é¢˜ï¼Œä½†äººèƒ½çŸ¥é“
            - allow split-brain operation (eventual consistency), such as Bayou and Dynamo
        - Raft works under all non-Byzantine conditions
            - either follow the Raft protocol correctly, or they halt
        - Raft may not preserve it across a leader change
            - the client will know that its request wasn't served, and will re-send it
            - the system has to be on its guard against duplicate requests
        - always a majority of all servers, dead and alive
        - randomize election timeouts
        - if more than half of the servers die
            - t will keep trying to elect a leader over and over
    - Raft æ‰€æœ‰è¯»å†™éƒ½ç»è¿‡ leaderã€‚åŠ èŠ‚ç‚¹å®ç°å®¹é”™ï¼Œå¹¶ä¸èƒ½å¢åŠ ååé‡ã€‚
    - raft
        - usage
            - fault-tolerant key/value database
            - fault-tolerant master
            - fault-tolerant locking service
        - read request and no-op
            - ä¸ºäº†ä¿è¯ linearizableï¼Œå¯ä»¥å…ˆç»™ follower å‘ no-op
            - ä¹Ÿå¯ä»¥ä½¿ç”¨ leaseï¼Œåœ¨ heartbeat ä¹‹åä¸€æ®µæ—¶é—´å†…ï¼Œä¸å…è®¸å˜æ›´
                - lease å†…ï¼Œå¯ä»¥ä¸å‘ no-op ç›´æ¥è¿”å›
                - the no other leader is allowed to be elected for the next 100 milliseconds
                - the leader can serve read-only requests for the next 100 milliseconds without further communication with the followers
            - åˆšæˆä¸º leader çš„æ—¶å€™ï¼Œéœ€è¦å‘å‡ æ¬¡ no-opï¼Œä¿è¯ committed log ä¸€è‡´
        - linearizability
            - the most common and intuitive definition formalizes behavior expected of a single server
            - an execution history is linearizable if one can find a total order of all operations, and in which each read sees the value from the write preceding it in the order.
        - duplicate RPC detection
            - case
                - server deaded, or request dropped (safe)
                - server executed but response dropped (dangerous)
            - detection
                - client picks an ID for each request
                    - same ID in re-sends of same RPC
                - k/v service maintains table indexed by ID
                    - record value after executing
                - when can we delete table entries
                    - one table per client
                    - client numbers RPCs sequentially
                    - client won't re-send older RPCs
                    - so server can forget client's lower entries
                - how does a new leader get the duplicate table
                    - all replicas update their duplicate tables as they execute command
                - if server crash, how does it restore its table
                    - from snapshot or replay the log to build table
        - read-only operations
            - read-only å‘é€ no-op æ˜¯ä¸ºäº†ä¿è¯å½“å‰èŠ‚ç‚¹ä»æ˜¯ leaderï¼Œå½“å‰çš„ committed log æ˜¯æœ€æ–°çš„
            - many applications are read-heavy. how to avoid commit for read-only operations?
            - idea: leases
                - a new leader cannot execute Put()s until previous lease period has expired
                - ä¿è¯ä¸ä¼šæœ‰æ–° leader ä¿®æ”¹æ•°æ®ï¼Œé‚£ä¹ˆè¿™æ®µæ—¶é—´å†…å³ä½¿å·²ç»ä¸æ˜¯ leaderï¼Œè¿”å›çš„æ•°æ®ä»æ˜¯æ­£ç¡®çš„
                - æ˜¯ä¸ºäº†ä¿è¯ linearizable
        - ğŸ¤”ï¸ ç–‘é—® åˆ†å¸ƒå¼é‡Œç»å¸¸ç”¨æ•°å­—åšåºå·ï¼Œæ•°å­—è‡ªå¢ï¼Œéƒ½ä¸è€ƒè™‘æº¢å‡ºå—ï¼Ÿ

---

## ZooKeeper

- ZooKeeper
    - a generic coordination service
        - widely-used replicated state machine service
        - inspired by Chubby (google's global lock service)
    - many applications in datacenter cluster need to coordinate
        - application that need a fault-tolerant "master" don't need to roll their own
        - GFS, MapReduce, load balancer, crawler, ...
    - high performance
        - asynchronous calls
        - allows pipelining
        - (faster than raft (100x faster
    - alternative
        - vs DNS, too slow, fail-over will take a long time
- API overview
    - operations are performed in global order
    - znode, the replicated object
        - hierarchy, named by pathnames
        - types
            - regular
            - empheral
            - sequential (name + seq_no)
        - metadata of application
            - configuration (server list + which is the primary)
            - timestamps
            - version number
    - sessions
        - client sign into ZooKeeper
        - client must send a heartbeat to the server to refresh session
            - ZooKeeper considers client "dead" if timeout
- operations on znodes
    - all operations (exclude sync) are asynchronous
    - all operations are FIFO-ordered per client
    - op: create/delete/get/set/exist/get_child/sync
- ordering guarantees
    - all write operations are totally ordered
    - all operations are FIFO-ordered per client
    - implications
        - read can return stale data
- not an end-to-end solution
    - with ZooKeeper
        - at least master is fault tolerant
        - won't run into split-brain problem
- implementation
    - overview
        - two layers: ZooKeeper service + ZAB layer
            - vs. KV service + Raft layer
    - duplicate requests
        - primary è¿”å›çš„ç»“æœä¸¢å¤±ï¼Œå®¢æˆ·ç«¯é‡è¯•
        - in Raft, use table to detect duplicates
        - in ZooKeeper
            - test-version-and-then-do-op
    - read operations
        - performance is slow if read ops go through Raft
            - æ¯æ¬¡éƒ½è¦ majority åŒæ„
        - read may return stale data if only master performs it
            - master å¯èƒ½ä¸å†æ˜¯ master äº†ï¼Œä½†è‡ªå·±ä¸çŸ¥é“ï¼ˆæ¯”å¦‚ç½‘ç»œæ•…éšœ
        - ZooKeeper: don't promise non-stale data
            - reads can be executed by any replica
                - can increase throughput by adding servers
            - read returns the last zxid it has seen
                - new primary can catch up to zxid before serving the read
                - avoids reading from past
        - sync-read guarantees data is not stale
            - sync optimization
                - avoid ZAB layer for sync-read
                - leader puts sync in queue between it and replica
                - in same spirit read optimization with raft
    - performance
        - reads inexpensive ï¼Œååé‡å’Œæœºå™¨æ•°æˆæ­£æ¯”
        - writes expensive ï¼Œååé‡å’Œæœºå™¨æ•°æˆåæ¯”
        - quick failure recovery
    - FAQ
        - linearizability and serializability
            - linearizability
                - used for systems without transactions
                - single-operation, single-object, real-time order
                - a real-time guarantee on the behavior of a set of single operations on a single object
                - linearizability is composable
                - atomic consistency, C in the CAP
            - serializability
                - used for systems that provide transactions
                - multi-operation, multi-object, arbitrary total order
                - the execution of a set of transactions over multiple items is equivalent to some serial execution (total ordering) of the transactions
                - serializability is not composable
                - I in the ACID (ACID çš„ C æè¿°å•ä¸ª transactionï¼ŒI æè¿°å¤šä¸ª txn ä¹‹é—´çš„å…³ç³»)
        - pipelining
            - ZK çš„ async API ä½¿ç”¨ callback çš„æ–¹å¼å‘Šè¯‰ client è°ƒç”¨ç»“æœ
            - ZooKeeper guarantees FIFO for client operations
        - fuzzy snapshots
            - doesn't require blocking all writes while the snapshot is made
            - construct a consistent snapshot by replaying the logs
            - all updates in ZooKeeper are idempotent and delivered in the same order
            - leader turn the operation into a transaction which is idempotent
        - leader election
            - ZAB (ZooKeeper Atomic Broadcast)

---

- æˆªæ­¢ç›®å‰åå¤å‡ºç°çš„ä¸»é¢˜
    - ä¸»ä»ç»“æ„ï¼Œå¤šå‰¯æœ¬
    - WAL æ—¥å¿—ã€å¿«ç…§ï¼Œç”¨äºå¼‚å¸¸æ¢å¤
    - å• master ä¿è¯ä¸€è‡´æ€§ã€æ—¶åº
    - å¤šæœºæ¨¡æ‹Ÿå•æœºè¡¨ç°ï¼Œæ ¹æ®å•æœºè¡Œä¸ºåˆ¤æ–­åˆ†å¸ƒå¼è¡Œä¸ºæ˜¯å¦ç¬¦åˆé¢„æœŸ

---


## Distributed Transactions

- transaction, hide interleaving and failure from application writers
    - atomic
    - serializable, transaction executed one by one
    - durable
- distributed transactions = concurrency control + atomic commit

- atomic commit by two-phase commit
    - transaction coordinator
        - PREPARE + COMMIT/ABORT
        - 2PC ç”± transaction coordinator å±…ä¸­åè°ƒ
        - æ‰€æœ‰èŠ‚ç‚¹éƒ½ prepare åˆ™ commitï¼Œä»»ä¸€èŠ‚ç‚¹å¤±è´¥éƒ½ abort
        - if node voted YES, it must "block": wait for TC decision
    - used by distributed databases for multi-server transactions
        - when a transaction uses data on multiple shards
    - slowï¼Œæ¯ä¸ªæ“ä½œéƒ½è¦æ±‚æ‰€æœ‰èŠ‚ç‚¹å‚ä¸ï¼Œå…¨éƒ¨æˆåŠŸï¼Œæ‰€ä»¥æ€§èƒ½å·®ï¼Œå¯é æ€§ä¹Ÿå·®
    - vs Raft
        - Raft é€šè¿‡ replicate å®ç° high availability
            - æ‰€æœ‰ server éƒ½æ‰§è¡Œç›¸åŒçš„æ“ä½œï¼Œä¿è¯ majority ä¸€è‡´
        - 2PC ä¿è¯æ•´ä¸ªç³»ç»Ÿçš„ä¸€è‡´æ€§
            - ä¸åŒ nodeï¼Œæ‰§è¡Œä¸åŒçš„æ“ä½œï¼ˆæ¯”å¦‚ä¸€ä¸ªäº‹åŠ¡çš„ä¸¤å¼ è¡¨ï¼Œæ‰§è¡Œä¸åŒæ“ä½œ
        - Raft çš„ä¸€è‡´å’Œ 2PC çš„ä¸€è‡´ï¼Œé’ˆå¯¹çš„ä¸æ˜¯ä¸€ä¸ªå±‚é¢çš„å¯¹è±¡
        - ä¸¤ä¸ªå¯ä»¥ä¸€èµ·ç”¨

- concurrency control
    - serial means one at a time, no parallel execution
        - å…·ä½“å®ç°å¯ä»¥å¹¶å‘ï¼Œä½†æ‰§è¡Œç»“æœè¦å’Œåºåˆ—æ‰§è¡Œä¸€è‡´
        - serializability lets programmer ignore concurrency
    - two classes
        - pessimistic, conflicts cause delays (waiting for locks)
        - optimistic, conflict causes abort+retry
- pessimistic concurrency control
    - two-phase lockingï¼Œè·å¾—é”ï¼Œé‡Šæ”¾é”
    - strong strict two-phase locking, locks until after commit/abort
        - serializable çš„å……åˆ†ä¸å¿…è¦æ¡ä»¶
    - 2PC çš„é”åœ¨ record ä¸Šï¼Œéƒ¨åˆ†åœºæ™¯æ¯” simple lockingï¼ˆå…¨è¡¨åŠ é”ï¼‰æ›´é«˜æ•ˆ
- optimistic concurrency control (OCC)
    - works best if few conflicts

- NVRAM, non-volatile RAM
    - RAM write is faster than SSD/HDD
    - write to f+1 machines to tolerate f failures
    - å¤‡ç”¨ç”µæ± ï¼Œæ–­ç”µåä»å¯è¿è¡Œå‡ åˆ†é’Ÿ
        - åœæ­¢å¤„ç†äº‹åŠ¡
        - RAM å†™å…¥ SSDï¼Œå¾…å¼€æœºåæ¢å¤
- performance bottleneck, network
    - kernel bypass

---

- build a reliable system out of unreliable components
- metrics
    - MTTF = mean time to failure           = 30 days = 43,200 minutes
    - MTTR = mean time to repair            = 10 minutes
    - availability = MTTF / (MTTF + MTTR)   = 43,200 / 43,210 = .9997
- transactions, which provide atomicity and isolation, while not hindering performance
    - atomicity: shadow copies vs. logs
    - isolation: two-phase locking
- distributed transactions: to run transactions across multiple machines
    - message loss, message re-ordering
        - reliable transport
        - exactly-once semantics
    - two-phase commit (2PC)
        - two phases: prepare phase, commit phase
        - client + coordinator + worker
        - abort if failure happened before commit point
        - retry if failure happened after commit point

---

## Spark

- Spark
    - restricted programming model, but more powerful than MapReduce
    - better programming model for iterative computations
        - MapReduce ä¸æ˜¯ä¸èƒ½è¡¨è¾¾ iterativeï¼Œä¸è¿‡éœ€è¦æ‹†åˆ†æˆå¤šä¸ª MapReduce ä»»åŠ¡ï¼Œæ¯” Spark å¤æ‚
    - targets batch, iterative applications
        - batch ä¹‹å¤–ï¼Œä¹Ÿæœ‰ streaming ç­‰åº”ç”¨å½¢æ€ï¼Œç„¶å spark ä¹Ÿæå‡ºäº† Streaming Spark
    - not good for build key/value store
    - can express MapReduce, Pregel
- performance
    - MapReduce uses replicated storage after reduce
    - Spark only spills to local disk
    - å¦‚æœè®¡ç®—ç¬¦åˆ MapReduce æ¨¡å‹ï¼Œæ¶æ„ä¸Šè®²ï¼ŒSpark å¹¶æ²¡ä¼˜åŠ¿
        - Spark's in-memory RDD caching will offer no benefit since no RDD is ever re-used
        - æ²¡æœ‰ä¼˜åŠ¿ï¼Œä½†è®¡ç®— MapReduce ç±»å‹çš„ä»»åŠ¡ä¹Ÿæ²¡æœ‰åŠ£åŠ¿
- Spark keep data in memory
- RDDs (Resilient Distributed Dataset)
    - immutable
    - support transformations and actions
        - transformations compute a new RDD from existing RDDs
        - transformations are lazy
        - transformation is a description of the computation
        - actions is used for when results are needed
    - key ideas behind RDDs
        - deterministic, lineage-based re-execution
        - the collections-oriented API
    - after Spark 2.0, RDDs are replaced by Dataset
- RDD lineage
    - Spark creates a lineage graph on an action
    - Spark uses the lineage to schedule job
- lineage and fault tolerance
    - one machine fails, we want to recompute only its state
    - the lineage tells us what to recompute
    - follow the lineage to identify all partitions needed
    - ç”¨æˆ·å¯ä»¥æŒ‡å®šå“ªäº› RDD éœ€è¦ replication
- RDD
    - list of partitions
    - list of (parent RDD, wide/narrow dependency)
        - wide, depends on serval parent partitions (eg, join)
        - narrow, depends on one parent partition (eg, map)
    - function to compute
    - partitioning scheme
    - computation placement hint
    - each RDD has location information associated with it, in its metadata

---








---

## Bayou

- ideas that are worth knowning
    - eventual consistency
    - conflict resolution
    - logging operations rather than data
    - use of timestamps to help agreement on order
    - version vectors
    - causal consistency via Lamport logical clocks
- ideas to remember
    - log of operations is equivalent to data
    - log helps eventual consistency (merge, order, and re-execute)
    - log helps conflict resolution (write operations easier than data)
    - causal consistency via Lamport-clock timestamps
    - quick log comparison via version vectors

- the log holds the truth; the DB is just an optimization
- ordered update log
    - syncing == ensure both devices have same log (same updates, same order)
    - DB is result of applying update functions in order
    - same log + same order = same DB content
- eventual consistency is the best you can do if you want to support disconnected operation
- timestamp
    - timestamp = (T, I)
    - T = creating device's wall-clock time
    - I = creating device's ID
- uses "Lamport logical clocks" for causal consistency
    - Lamport clock
        - Tmax = highest timestamp seen from any device
        - T = max(Tmax + 1, wall-clock time)
    - ä¿è¯æ–°çš„ timestamp ä¸ä¼šæ¯”ä¹‹å‰å°
- "primary replica" to commit updates
    - one device is the "primary replica"
    - primary marks each received update with a Commit Sequence Number
- anti-entropy
- version vector
    - a summary of state known by a participant
- discard committed updates from log
    - keep a copy of the DB as of the highest known CSN
    - never need to roll back farther

---

## Naiad

- streaming and incremental computation
- Naiad vs Spark
    - Spark improved performance for iterative computations
    - better, incremental and streaming computations
    - equal (maybe better), batch and iterative processing
    - worse, interactive queries
    - worse, Spark is well-integrated with existed systems
- incremental processing
    - fixed data-flow
    - input vertices
        - file or stream of events
        - inject records into graph
    - stateful vertices
        - like cached RDDs but mutable
- data-flow cycles
    - vs Spark
        - Spark has no cycles (DAG)
        - iterate by adding new RDDs
        - no RDD can depend on its own child
    - loop contexts
        - can nest arbitrarily
        - cannot partially overlap
- ordering
    - avoid time-travelling updates
    - timestamps form a partial order
    - timely dataflow is low-level infrastructure
- low-level vertex API
    - deal with timestamps
    - different strategies
        - incrementally release records for a time, finish up with notification
        - buffer all records for a time, then release on notification
- progress tracking
    - protocol to figure out when to deliver notifications to vertices
    - single-threaded
    - distributed
        - aggregate events locally before broadcasting
        - global aggregator merges updates from different workers
- fault tolerance
    - log all messages to disk before sending
        - high common-case overhead
    - write globally synchronous, coordinated checkpoints
        - induces pause times while making checkpoints

---









---

## P2P, DHT

- decentralized systems
    - build reliable systems out of many unreliable computers
    - shift control/power from organizations to users
- peer-to-peer
    - spreads network/caching costs over users
    - advantage: è®¾å¤‡æˆæœ¬ä½ï¼Œå•ç”¨æˆ·æ•…éšœä¸å½±å“ç³»ç»Ÿï¼Œæ¯ä¸ªç”¨æˆ·çš„è´Ÿæ‹…éƒ½ä¸å¤§
    - disadvantage: æ•°æ®åˆ†æ•£ä¸å¥½æ‰¾ï¼Œç”¨æˆ·æœºå™¨ä¸å¯é ï¼Œå¼€æ”¾çš„ç½‘ç»œæœ‰è¢«æ”»å‡»çš„é£é™©
    - usage: file sharing, chat, bitcoin

- BitTorrent
    - the tracker is a weak part of the design
        - torrent file with content hash and IP address of tracker
        - app talks to tracker
    - tracker may not be reliable
- DHT (distributed hash table)
    - DHT is more reliable than tracker
    - DHT a decentralized key/value store
    - DHT is weak consistency
    - Kademlia
        - the key is the torrent file content hash
        - the value is the IP address of peers
    - each node has references to only a few other nodes
    - lookups traverse the data structure

- Chord, peer-to-peer lookup system
    - Kad (Kademlia) is inspired by Chord
    - topology
        - ring, all IDs are 160bit numbers
        - each node has an ID, hash(IP address)
        - each key has an ID, hash(key)
    - key_ID and node_ID
        - key is stored at the key ID's successor node
        - closeness is defined as the "clockwise distance"
        - å¦‚æœ hash ä¿è¯å‡åŒ€åˆ†å¸ƒï¼Œé‚£ä¹ˆèŠ‚ç‚¹çš„è´Ÿè½½å°±æ˜¯å‡è¡¡çš„
    - routing
        - basic
            - each node knows its successor on the ring
            - forward query in a clockwise direction until done
            - data structure is a linked list, linear search is slow
        - log(n) finger table
            - each node keep a finger table containing up to M nodes
            - periodically looks up each finger to maintain table
        - why not binary tree
            - hot-spot at the root
            - failure on root would be a big problem
            - finger table distributes the load
    - stabilization
        - each node keeps track of its current predecessor
    - node failures
        - recover from dead next hop by using next-closest finger-table entry

---

## Dynamo

- Database, eventually consistent, write any replica
    - Like Bayou, with reconciliation
    - Like Parameter Server, but geo-distributed
- SLA, constant failures, always writeable
- big picture: each item replicated at a few random nodes, by key hash

- consistent hashing
    - node_ID = random
    - key_ID = MD5(key)
    - coordinator: successor of key. (clients send put/get to coordinator)
    - preference list: replicas at successors
    - coordinator forwards put/get to nodes on the preference list
- failures
    - temporary, store new puts elsewhere until node is available
    - permanent, make new replica of all content
    - Dynamo itself treats all failures as temporary
        - administrator can remove node permanent
- always writeable
    - no master: sloppy quorums
    - conflicting versions (when failures happend): eventual consistency
- sloppy quorum
    - quorum: R+W > N
        - never wait for all N
    - sloppy quorum means R/W overlap not guaranteed
    - coordinator
        - sends put/get to first N reachable nodes, in parallel
        - put: waits for W replies
        - get: waits for R replies
- eventual consistency
    - allow reads to see stale or conflicting data
    - if conflicts, reader must merge and then write
    - no atomic operations (eg. CAS)
- version vectors
    - Dynamo deletes least-recently-updated entry (threshold=10 now)
- N, R, W
    - n3r2w2, default, reasonable fast R/W, reasonable durability
    - n3r1w3, fast R, slow W, durable

- anti-entropy using Merkle trees
    - to determine what is different between two replicas, Dynamo traverses a Merkle representation of the two replicas
- gossip protocol
    - a system that doesn't have a master that knows about all participants in the system
    - gossip protocol: the protocol used in such system to to find other members

---

## Bitcoin

- bitcoin
    - a digital currency
    - a public ledger to prevent double-spending
    - no centralized trust or mechanism
    - malicious users (Byzantine faults)
- problem
    - forgery
    - double spending
    - theft
- idea, signed sequence of transactions
- transaction record contains
    - pub(user1), public key of new owner
    - hash(prev), hash of this coin's previous transaction record
    - sig(user2), signature by previous owner's private key
- forgery
    - current owner's private key needed to sign next transaction
- block chain
    - goal, agreement on transaction log to prevent double-spending
    - ç–‘é—®ã€‚æŸ¥æ‰€æœ‰äº¤æ˜“è®°å½•ï¼Œè¦æ‹¥æœ‰å®Œæ•´çš„æ—¥å¿—ã€‚è¿™ä¸ªæŸ¥è¯¢æ•ˆç‡æ˜¯æ€ä¹ˆä¿è¯çš„ï¼Ÿ
    - the block chain contains transactions on all coins
    - payee doesn't believe transaction until it's in the block chain
    - block contains
        - hash(prev_block)
        - transactions
        - nonce
        - current time (wall clock timestamp)
    - create new block
        - requirement: hash(block) has N leading zeros
        - try nonce values until this works out
            - æ˜“éªŒè¯ï¼Œéš¾ä¼ªé€ 
    - fork
        - switch to longer chain if peers become aware of one
        - temporary double spending is possible, due to forks

