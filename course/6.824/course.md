# Distributed Systems

https://pdos.csail.mit.edu/6.824/schedule.html

---

## Introduction

- why distributed
    - to achieve security via isolation
    - to tolerate faults via replication
    - to scale up throughput via parallel CPUs/mem/disk/net
- topic
    - implementation
    - performance
    - fault tolerance
    - consistency

- MapReduce
    - overview
        - åŸºæœ¬æ€è·¯ã€é€‚ç”¨åœºæ™¯
        - è§£å†³ä»€ä¹ˆé—®é¢˜ï¼Œå¦‚ä½•è§£å†³
    - advantage
        - hide painful details, scale well
        - æ¯”å…¶ä»–æ–¹æ¡ˆä¼˜åŠ¿åœ¨å“ª
    - limitations
        - å“ªäº›åœºæ™¯ä¸é€‚ç”¨ï¼Œä¸ºä»€ä¹ˆä¸é€‚ç”¨
        - æ€§èƒ½ã€æ•ˆç‡çš„ç“¶é¢ˆåœ¨å“ª
    - implementation details
        - å¦‚ä½•åº”å¯¹ limitation
    - fault tolerance, crash recovery
        - å‘ç°ã€å¤„ç†ã€æ¢å¤

---

## Infrastructure: RPC and threads

- threading challenages
    - sharing data
        - synchronization
    - coordination
    - granularity
        - coarse-grained
        - fine-grained
- sharing+locks vs channels
    - state vs communication
- RPC
    - client/server communication
- RPC failure
    - failure-handling scheme
        - best effort (retry)
        - at most once (unique ID for each request)
            - how to create unique ID
            - when to discard old RPC response (heartbeat, received next UID)
        - exactly once

---

## GFS

- system design
    - trading consistency for simplicity and performance
    - performance, fault-tolerance, consistency
- consistency
    - replicate
        - multiply machines, reads and writes go across
    - strong consistency
        - bad for performance, easy for application writers
    - weak consistency
        - good for performance, easy for scale, bad for reason about
- consistency model
    - a replicated FS behaved like a non-replicated FS
        - what happens on a single machine?
    - challenges
        - concurrency
        - machine failures
        - network partitions

- GFS
    - TODO
    - read
        - 64MB chunk
        - 3x replication
        - master server
        - worker server
    - write

---

## Primary/Backup Replication

- ideal properties
    available: still useable despite  some class of failures
    strongly consistent: looks just like a single server to clients
    transparent to clients
    transparent to server software
    efficient
- fault tolerance, replication
    - two or more servers, if one replica fails, others can continue
- question
    - what state to replicate
    - when to cut over to backup
    - does primary have to wait for backup
- approaches
    - state transfer
        - primary executes the operations, sends new state to backups
        - more simpler
    - replicated state machine
        - all replicas execute all operations
        - more efficient
            - operations are small compared to data
            - complex to get right (same start state, same operations, same order, deterministic)

- Fault-Tolerant Virtual Machines
    - two machines, primary and backup
    - primary sends all inputs to backup over logging channel
    - the backup must lag by one event (one log entry)

---

## Raft

- fault-tolerant services using replicated state machines
    - same client-visible behavior as single non-replicated server
    - available despite some number of failed servers
    - each replica server executes same commands in same order
- how to avoid split brain
    - èŠ‚ç‚¹æ•…éšœäº†ï¼Œåº”è¯¥å¿½ç•¥æ•…éšœèŠ‚ç‚¹ï¼›ç½‘ç»œæ•…éšœäº†ï¼Œåº”è¯¥ç­‰å¾…ç½‘ç»œæ¢å¤
    - æ²¡æœ‰ç­‰å¾…ç½‘ç»œæ¢å¤ï¼Œç»“æœå°±æ˜¯ split brain
    - computers cannot distinguish crashed machines vs a partitioned network
- majority vote
    - 2f+1 servers to tolerate f failures (3 servers can tolerate 1 failure)
    - must get majority (f+1) of servers ti agree to make progress
        - majority is out of all 2f+1 servers, not just out of live ones
    - å°‘æ•°æœä»å¤šæ•°ï¼Œå§‹ç»ˆä¿è¯ f+1 ä¸ªèŠ‚ç‚¹çš„çŠ¶æ€æ˜¯ä¸€è‡´çš„ï¼Œä¹Ÿå°±æ˜¯æ­£ç¡®çš„çŠ¶æ€
    - å­˜æ´»çš„èŠ‚ç‚¹å°‘äº f+1 ä¸ªï¼Œé‚£ä¹ˆæŠ•ç¥¨å§‹ç»ˆæ˜¯å¤±è´¥çš„
    - ï¼ˆç½‘ç»œçœŸçš„åäº†ï¼Œé‚£ä¹ˆè¿™å°±æ˜¯ä¸ªç‰ºç‰²å¯ç”¨æ€§æ¢å–ä¸€è‡´æ€§çš„æ–¹æ¡ˆ

- goal
    - same client-visible behavior as single non-replicated server
    - available despite minority of failed/disconnected servers

- two partition-tolerant replication schemes, Paxos and View-Stamped Replication

- Raft
    - elections, log handling, persistence, client behavior, snapshots
    - overview
        - diagram: clients, k/v layer, Raft layer
            - Raft é€‰å‡º leader
            - client å‘é€æ“ä½œåˆ° leader çš„ kv
            - Raft å°†æ“ä½œå‘é€åˆ° followersï¼Œfollowers è®°å½•åˆ° local log å¹¶è¿”å›ç»™ leader
            - leader ç­‰ majority æ“ä½œæˆåŠŸï¼Œæ‰€æœ‰ replica å¼€å§‹æ“ä½œ kv
            - leader å°† kv çš„æ“ä½œç»“æœè¿”å›ç»™ client
        - log
            - log éƒ½æ˜¯æœ‰ç¼–å·çš„
            - å¯ä»¥ç¡®å®šæ“ä½œçš„æ‰§è¡Œé¡ºåº
            - åŒæ—¶ä¿è¯æ‰€æœ‰èŠ‚ç‚¹çŠ¶æ€ä¸€è‡´
    - leader election
        - client åªå’Œ leader äº¤äº’ï¼Œå€Ÿæ­¤ä¿è¯æ“ä½œçš„æ‰§è¡Œé¡ºåº
        - terms (number)
            - æ¯ä¸ª term å¯ä»¥æœ‰ä¸€ä¸ª leaderï¼ˆä¹Ÿå¯èƒ½é€‰ä¸»å¤±è´¥æ²¡æœ‰ leader
                - leader è¦æœ‰ majority åŒæ„ï¼Œæ‰€ä»¥èƒ½ä¿è¯æœ€å¤šåªæœ‰ä¸€ä¸ª leader
            - è°å¤§è°å°±æ˜¯ leaderï¼Œè°å¤§è°å°±è¯´äº†ç®—
        - when
            - leader å‘é€ heartbeat åˆ° followers
            - å¦‚æœ followers è¿‡äº†ä¸€æ®µæ—¶é—´è¿˜æ²¡æ”¶åˆ°ï¼Œå°±ä¼šè®¤ä¸º leader æ•…éšœäº†ï¼Œå¼€å§‹æ–°çš„é€‰ä¸»ç¨‹åºï¼ˆterm å¢å¤§
            - ï¼ˆè¿™ç§ç©æ³•å¯èƒ½å¯¼è‡´ä¸å¿…è¦çš„é€‰ä¸»ï¼Œä½†æ˜¯é›†ç¾¤æ•°æ®çš„æ­£ç¡®æ€§æ˜¯æœ‰ä¿è¯çš„
            - å¯èƒ½ç”±äºå…¶ä»–é—®é¢˜é€‰ä¸»å¤±è´¥ã€‚å¤±è´¥äº†ä¼šå†æ¬¡è¿›å…¥é€‰ä¸»æµç¨‹ï¼ˆterm æœ€å¤§é‚£ä¸ªå°è¯•å‘èµ·æ–°çš„é€‰ä¸»æµç¨‹
                - majority çš„èŠ‚ç‚¹éƒ½ä¸å¯ç”¨ï¼Œé€‰ä¸»ä¼šå¤±è´¥ï¼ˆä¸ç®¡æ˜¯çœŸçš„ä¸å¯èƒ½ï¼Œè¿˜æ˜¯ç½‘ç»œæ•…éšœï¼Œåæ­£æ²¡ majority åŒæ„å°±æ— æ³•è½¬å˜ä¸º leader
                - å¤šä¸ªèŠ‚ç‚¹éƒ½è¯•å›¾å˜æˆ leaderï¼Œå¯èƒ½ç”±äºå¹¶å‘å¯¼è‡´æŠ•ç¥¨å¤±è´¥ï¼ˆè¿˜æ˜¯æ²¡è¾¾åˆ° majority åŒæ„ï¼Œä½†èŠ‚ç‚¹è¿˜èƒ½æ­£å¸¸é€šä¿¡
            - å¾—åˆ° majority æ”¯æŒçš„èŠ‚ç‚¹ï¼Œå‘å¿ƒè·³ç»™æ‰€æœ‰èŠ‚ç‚¹ï¼Œè®©å¤§å®¶çŸ¥é“é€‰ä¸»å·²ç»å®Œæˆäº†
        - candidate
            - å°è¯•è½¬æ¢ä¸º leader çš„èŠ‚ç‚¹ï¼Œè¿›å…¥ candidate çŠ¶æ€
            - majority æ”¯æŒï¼Œå°±å˜æˆ leader
            - majority æ”¯æŒäº†å…¶ä»– candidateï¼Œå°±å˜å› followers
            - é€‰ä¸»å¤±è´¥ï¼Œä¿æŒ candidate çŠ¶æ€ï¼Œç­‰ timeout åé‡æ¥
        - timeout
            - randomï¼Œé¿å…å¤šä¸ª followers åŒæ—¶å°è¯•è½¬æ¢ä¸º candidate
            - lowest random delay
            - ç½‘ç»œå¯èƒ½å„ç§é—®é¢˜ï¼Œæ‰€ä»¥ timeout è‡³å°‘è¦å¤Ÿå‡ ä¸ª heartbeat æ¥å›
            - é€‰ä¸»ä¹Ÿè¦æ—¶é—´ï¼Œæ‰€ä»¥ timeout è‡³å°‘ä¹Ÿè¦å¤Ÿæ•´ä¸ªç³»ç»Ÿå®Œæˆä¸€æ¬¡é€‰ä¸»
    - Raft log
        - replicated vs. committed entries
            - committed è¯´æ˜ majority åŒæ„ï¼Œä¸ä¼šå˜äº†
            - replicated åªæ˜¯å®Œæˆå¤åˆ¶ï¼Œå¯èƒ½ä¼šå˜
        - servers can temporarily have different log
        - client åªå’Œ leader äº¤äº’ã€‚é‚£ä¹ˆ leader å‘ç”Ÿæ›´æ¢çš„æ—¶å€™ï¼Œclient ä¼šæ€ä¹ˆæ ·
        - æ›´æ¢ leader ä¹‹åï¼Œserver çš„ log å¯èƒ½å¤„åœ¨ä¸åŒçŠ¶æ€
            - force followers adopt new leader's log
            - replicated å¯èƒ½å‡ºç°å›æ»š
            - è¦ä¿è¯ä¸èƒ½å½±å“ committed çš„çŠ¶æ€
        - Raft needs to ensure elected leader has all committed log entries
            - at least as up to date
    - persistence
        - repair soon after crash to avoid dipping below a majority
        - strategies
            - ä¿®å¤é—®é¢˜ï¼Œé‡å¯èŠ‚ç‚¹ï¼ˆæ•°æ®å¯èƒ½è¿‡æœŸï¼Œä½†å¤§éƒ¨åˆ†è¿˜åœ¨
            - å¯ç”¨æ–°èŠ‚ç‚¹ï¼ˆéœ€è¦å¤åˆ¶ä¹‹å‰çš„æ•°æ®
        - persistent state
            - logList, å¦‚æœå®•æœºä¹‹å‰å±äº majority ä¸€éƒ¨åˆ†ï¼Œé‚£ä¹ˆé‡Œé¢åŒ…å« committed çš„æ•°æ®ï¼Œè¦è®°ä¸‹ä½œä¸ºè¯æ®å•Š
            - votedFor, ä¸è®¸ç»™ä¸åŒçš„ term æŠ•èµæˆç¥¨ï¼Œæ‰€ä»¥è¦è®°ä¸‹
            - currentTerm, term åªè®¸å¢ä¸è®¸å‡ï¼Œæ‰€ä»¥è¦è®°ä¸‹ä¹‹å‰çš„
        - bottleneck for performance
            - æŒä¹…åŒ–åˆ°ç¡¬ç›˜ï¼Œè¦æ—¶é—´å•Š
    - snapshots, log compaction
        - æ¢å¤çŠ¶æ€æ—¶ï¼Œå¦‚æœé‡æ”¾å®Œæ•´æ—¥å¿—ï¼Œå¤ªæ…¢
        - log å¯èƒ½æ¯” state å¤§å¾—å¤šï¼Œæ‰€ä»¥å¯ä»¥å– state çš„å¿«ç…§ï¼Œå‡å°‘éœ€è¦é‡æ”¾çš„æ—¥å¿—æ•°é‡
        - åš state å¿«ç…§çš„æ—¶å€™ï¼Œæœª committed çš„ log è¦ä¿ç•™ï¼ˆå¯èƒ½è¢«ç®—åœ¨ majority é‡Œäº†
        - snapshot reflects only committed entries
        - service's on-disk state = service's snapshot + Raft's persistent log
    - configuration change
        - configuration = set of servers
            - å¢åˆ èŠ‚ç‚¹
        - problem
            - followers will learn new configuration at different times
            - ä¼šç»™è®¡ç®— majority é€ æˆé—®é¢˜
            - ï¼ˆä¹Ÿè¦æ±‚ majority åŒæ„å³å¯ï¼Ÿé‚£ä¹ˆèŠ‚ç‚¹å®•æœºçš„æƒ…å†µï¼Œæ°¸è¿œæ— æ³•è¾¾æˆ majority?
        - joint consensus
            - avoids any time when both old and new can choose leader independently
            - administrator asks the leader to switch to new configuration
            - leader å°†æ–°é…ç½®ä¸‹å‘åˆ° followersï¼Œæ–°æ—§é…ç½®éƒ½ committed ä¹‹åï¼Œå†åˆ‡æ¢åˆ°æ–°é…ç½®
    - performance
        - many situations don't require high performance
        - (Raft) sacrifice performance for simplicity
    - faq
        - sacrifice performance for simplicity
            - æ“ä½œè¦æŒä¹…åŒ–åˆ°ç¡¬ç›˜
            - ä¹±åºåˆ°è¾¾çš„æ¶ˆæ¯ï¼Œä¼šè¢«ä¸¢å¼ƒ
            - å¿«ç…§å¯èƒ½å¾ˆå¤§ï¼ˆåˆ›å»ºã€ä¼ è¾“éƒ½æ…¢ï¼‰
            - æ—¥å¿—è¦æ±‚æœ‰åºï¼Œå¯èƒ½å¯¼è‡´æ— æ³•åˆ©ç”¨å¤šæ ¸
        - While Paxos requires some thought to understand, it is far simpler than Raft
            - But Paxos solves a much smaller problem than Raft
        - real-world systems are derived from Paxos
            - Chubby, Spanner, Megastore, ZooKeeper/ZAB
        - real-world users of Raft
            - docker, etcd, CockroachDB, RethinkDB, TiKV
        - systems can survive and continue to operate when only a minority of the cluster is active
            - do it with different assumptions, or different client-visible semantics
            - human decideï¼Œæœºå™¨ä¸çŸ¥é“å®•æœºè¿˜æ˜¯ç½‘ç»œé—®é¢˜ï¼Œä½†äººèƒ½çŸ¥é“
            - allow split-brain operation (eventual consistency), such as Bayou and Dynamo
        - Raft works under all non-Byzantine conditions
            - either follow the Raft protocol correctly, or they halt
        - Raft may not preserve it across a leader change
            - the client will know that its request wasn't served, and will re-send it
            - the system has to be on its guard against duplicate requests
        - always a majority of all servers, dead and alive
        - randomize election timeouts
        - if more than half of the servers die
            - t will keep trying to elect a leader over and over
    - Raft æ‰€æœ‰è¯»å†™éƒ½ç»è¿‡ leaderã€‚åŠ èŠ‚ç‚¹å®ç°å®¹é”™ï¼Œå¹¶ä¸èƒ½å¢åŠ ååé‡ã€‚
    - raft
        - usage
            - fault-tolerant key/value database
            - fault-tolerant master
            - fault-tolerant locking service
        - read request and no-op
            - ä¸ºäº†ä¿è¯ linearizableï¼Œå¯ä»¥å…ˆç»™ follower å‘ no-op
            - ä¹Ÿå¯ä»¥ä½¿ç”¨ leaseï¼Œåœ¨ heartbeat ä¹‹åä¸€æ®µæ—¶é—´å†…ï¼Œä¸å…è®¸å˜æ›´
                - lease å†…ï¼Œå¯ä»¥ä¸å‘ no-op ç›´æ¥è¿”å›
                - the no other leader is allowed to be elected for the next 100 milliseconds
                - the leader can serve read-only requests for the next 100 milliseconds without further communication with the followers
            - åˆšæˆä¸º leader çš„æ—¶å€™ï¼Œéœ€è¦å‘å‡ æ¬¡ no-opï¼Œä¿è¯ committed log ä¸€è‡´
        - linearizability
            - the most common and intuitive definition formalizes behavior expected of a single server
            - an execution history is linearizable if one can find a total order of all operations, and in which each read sees the value from the write preceding it in the order.
        - duplicate RPC detection
            - case
                - server deaded, or request dropped (safe)
                - server executed but response dropped (dangerous)
            - detection
                - client picks an ID for each request
                    - same ID in re-sends of same RPC
                - k/v service maintains table indexed by ID
                    - record value after executing
                - when can we delete table entries
                    - one table per client
                    - client numbers RPCs sequentially
                    - client won't re-send older RPCs
                    - so server can forget client's lower entries
                - how does a new leader get the duplicate table
                    - all replicas update their duplicate tables as they execute command
                - if server crash, how does it restore its table
                    - from snapshot or replay the log to build table
        - read-only operations
            - read-only å‘é€ no-op æ˜¯ä¸ºäº†ä¿è¯å½“å‰èŠ‚ç‚¹ä»æ˜¯ leaderï¼Œå½“å‰çš„ committed log æ˜¯æœ€æ–°çš„
            - many applications are read-heavy. how to avoid commit for read-only operations?
            - idea: leases
                - a new leader cannot execute Put()s until previous lease period has expired
                - ä¿è¯ä¸ä¼šæœ‰æ–° leader ä¿®æ”¹æ•°æ®ï¼Œé‚£ä¹ˆè¿™æ®µæ—¶é—´å†…å³ä½¿å·²ç»ä¸æ˜¯ leaderï¼Œè¿”å›çš„æ•°æ®ä»æ˜¯æ­£ç¡®çš„
                - æ˜¯ä¸ºäº†ä¿è¯ linearizable
        - ğŸ¤”ï¸ ç–‘é—® åˆ†å¸ƒå¼é‡Œç»å¸¸ç”¨æ•°å­—åšåºå·ï¼Œæ•°å­—è‡ªå¢ï¼Œéƒ½ä¸è€ƒè™‘æº¢å‡ºå—ï¼Ÿ

---

## ZooKeeper

- ZooKeeper
    - a generic coordination service
        - widely-used replicated state machine service
        - inspired by Chubby (google's global lock service)
    - many applications in datacenter cluster need to coordinate
        - application that need a fault-tolerant "master" don't need to roll their own
        - GFS, MapReduce, load balancer, crawler, ...
    - high performance
        - asynchronous calls
        - allows pipelining
        - (faster than raft (100x faster
    - alternative
        - vs DNS, too slow, fail-over will take a long time
- API overview
    - operations are performed in global order
    - znode, the replicated object
        - hierarchy, named by pathnames
        - types
            - regular
            - empheral
            - sequential (name + seq_no)
        - metadata of application
            - configuration (server list + which is the primary)
            - timestamps
            - version number
    - sessions
        - client sign into ZooKeeper
        - client must send a heartbeat to the server to refresh session
            - ZooKeeper considers client "dead" if timeout
- operations on znodes
    - all operations (exclude sync) are asynchronous
    - all operations are FIFO-ordered per client
    - op: create/delete/get/set/exist/get_child/sync
- ordering guarantees
    - all write operations are totally ordered
    - all operations are FIFO-ordered per client
    - implications
        - read can return stale data
- not an end-to-end solution
    - with ZooKeeper
        - at least master is fault tolerant
        - won't run into split-brain problem
- implementation
    - overview
        - two layers: ZooKeeper service + ZAB layer
            - vs. KV service + Raft layer
    - duplicate requests
        - primary è¿”å›çš„ç»“æœä¸¢å¤±ï¼Œå®¢æˆ·ç«¯é‡è¯•
        - in Raft, use table to detect duplicates
        - in ZooKeeper
            - test-version-and-then-do-op
    - read operations
        - performance is slow if read ops go through Raft
            - æ¯æ¬¡éƒ½è¦ majority åŒæ„
        - read may return stale data if only master performs it
            - master å¯èƒ½ä¸å†æ˜¯ master äº†ï¼Œä½†è‡ªå·±ä¸çŸ¥é“ï¼ˆæ¯”å¦‚ç½‘ç»œæ•…éšœ
        - ZooKeeper: don't promise non-stale data
            - reads can be executed by any replica
                - can increase throughput by adding servers
            - read returns the last zxid it has seen
                - new primary can catch up to zxid before serving the read
                - avoids reading from past
        - sync-read guarantees data is not stale
            - sync optimization
                - avoid ZAB layer for sync-read
                - leader puts sync in queue between it and replica
                - in same spirit read optimization with raft
    - performance
        - reads inexpensive ï¼Œååé‡å’Œæœºå™¨æ•°æˆæ­£æ¯”
        - writes expensive ï¼Œååé‡å’Œæœºå™¨æ•°æˆåæ¯”
        - quick failure recovery

---

- æˆªæ­¢ç›®å‰åå¤å‡ºç°çš„ä¸»é¢˜
    - ä¸»ä»ç»“æ„ï¼Œå¤šå‰¯æœ¬
    - WAL æ—¥å¿—ã€å¿«ç…§ï¼Œç”¨äºå¼‚å¸¸æ¢å¤
    - å• master ä¿è¯ä¸€è‡´æ€§ã€æ—¶åº
    - å¤šæœºæ¨¡æ‹Ÿå•æœºè¡¨ç°ï¼Œæ ¹æ®å•æœºè¡Œä¸ºåˆ¤æ–­åˆ†å¸ƒå¼è¡Œä¸ºæ˜¯å¦ç¬¦åˆé¢„æœŸ

---


