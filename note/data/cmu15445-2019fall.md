# CMU-DB 15445 (2019 fall)

---

## Database Storage

- engine -> buffer pool (memory) -> storage manager (disk)
- database pages
    - directory file (map page_id to page)
    - slotted page: page header, slot array, tuples
    - tuple: tuple header, byte sequence
        - tuple_id = (page_id, offset)
- others
    - why not mmap
    - log-structured
    - how to represent data (int, float, string, datetime, blob)
    - OLAP vs OLTP, use case
    - row store vs column store, use case, advantages and disadvantages

---

## Buffer Pool

- buffer pool vs OS page cache
- buffer pool: page_table(map page_id to frame) + frame(page)
- evict policy
    - LRU
    - CLOCK
- dirty flag, write back to disk
- optimization
    - multiple pool, different pool for pages of different type (tuple, index, ...)
- lock vs latch
    - lock for high-level, for example, transaction
    - latch for low-level, for example, buffer pool page

---

http://www.interdb.jp/pg/pgsql08.html

PG 里使用了 clock sweep (a variant of Not Frequently Used) 来处理不再使用的缓存。
顺序读取很大的 table，可能导致大量 buffer 失效。
为了避免这种问题，PG 在部分场景不使用 buffer pool，而是一个独立的 ring buffer。

---

## B+ tree index

- operation (insert, delete)
- design decision
    - duplicate key
- optimization
- usage
    - implicit index
    - partial index

---

- radix tree
- inverted index, for full-text search

---

- 又讲了一次 lock vs latch
- 在 DBMS 里，lock 是指 transaction 的锁，latch 是指 data structure 的锁
- 数据库里似乎都是用读写锁

---

- btree + latch
- latch crabbing / coupling
    - 按 btree 的层次处理锁
        - 关键在于，哪些情况下，可以释放父节点的锁
        - 也就是，哪些情况下，子节点的修改，不会影响到父节点
    - 优化（乐观锁
        - insert/delete 的时候，总是要拿到写锁，影响性能
        - 先按 read 的方式拿读锁，只在 leaf node 拿写锁
        - 如果不安全，再重新全程写锁
    - 在 leaf node 处理 range scan 的时候，要小心死锁
        - range scan 是横向拿锁，而latch crabbing 是纵向拿锁，可能会出问题

---

## operator execution

### sort by

- in-memory sort
    - quick sort
- external sort
    - merge sort
    - double buffer optimization (在 IO 时利用好 CPU 资源

### group by, distinct

- hashing do more sequential IO

- if sort is not required, hash is a better choice
    - external hash

### join

- join 操作，就是找出两个列表的重复项。嵌套的两层循环、先排序再比较、计算 hash，数据库用了这三种算法。
- DBMS 特别的一点是，需要考虑硬盘 IO 的次数，以及可用内存的大小

- nested loop join
    - use the smaller table as the outer table
    - optimization
        - block nested loop join
            - 不是每次处理一个 tuple，而是每次处理一个 block
- sort-merge join
- hash join
    - 如果内存放不下整个 hash table，要怎么优化

---

## query

- processing model
    - iterator model / pipeline model / volcano model
        - the most common
        - `child.next()`, get a tuple from child operator
        - `emit(tuple)`, yield a tuple to parent operator
        - 一些有状态的操作，会等所有 tuple 返回。比如 join, subquery, order by 什么的，被称为 pipeline breakers
    - materialization model
        - ideal for OLTP, in-memory DBMS
        - `child.output()`, get all tuples from child operator
        - `return [tuple, ...]`, return all tuples to parent operator
        - 一次性把全部数据都放到内存里，适合一次操作少量数据的场景（也就是 OLTP 啦
            - 减少了反复调用 `next()` 的开销
    - vectorized model / batch model
        - like iterator model, but batch
        - ideal for OLAP
        - `child.next()`, get N tuples from child operator
        - `emit([tuple, ...])`, yield N tuples to parent operator
        - 一次获取多行数据，所以叫 batch。（也是为了减少 `next()` 调用的开销

- access method
    - sequential scan
        - optimization
            - zone maps
                - 提前计算好 page 的最大最小值等信息
                - 如果碰巧能参与 where 的条件判断，可以跳过不需要的 page
    - index scan
    - multi-index scan / bitmap scan
        - 一个 query 匹配了多个 index，对结果取交集（可以使用 bitmap/hash_table 等方式

- parallel execution
    - multi-process, multi-threaded
    - IO
        - replication (transparent to the DMBS
        - partition (transparent to the application, ideally
    - query
        - inter-query
            - multi-query in parallel
            - increase throughput, reduce latency
            - lock
        - intra-query
            - one-query, multi-operator in parallel
            - decrease latency for long-running query
            - approach
                - horizontal （数据分成多份，并行处理，然后聚合
                - vertical （不同 worker 处理不同 operator（感觉好像和普通方式没啥区别？
                    - pipelined parallelism
                    - used in stream processing systems

---

## query optimization

- rule based
    - alse called query rewriting
    - based on relational algebra equivalence
    - selection pushdown （减少后续参与计算的的 tuple 数量
    - projection pushdown （减少后续需要传递的 column 数量
    - expression simplification （优化无用的计算

- cost based
    - statistic about database data

---

## concurrency control

### ACID

- transaction 一切的核心都是事务

- ACID
    - atomicity 事务要么 commit 要么 abort
    - consistency 对分布式系统更有意义
    - isolation 事务之间的隔离
    - durability 事务 commit 之后，即使数据库宕机也能恢复回来

- atomicity
    - logging
    - shadow paging 所有东西复制一份，commit 覆盖原数据，abort 删除副本
- consistency
    - transaction consistency is the application's responsibility
    - 在分布式系统中，一个节点上 commit 之后，其他节点应该同时 commit
        - 这里说分布式系统，其实更接近 CAP 中的 consistency
        - 单机的 consistency 到底是什么，现在有点不明白了……
        - https://dba.stackexchange.com/a/31261 说 ACID 中的 C 指多个 connection 读取的数据是一致的
- isolation
    - a model that easier to reason about
    - concurrency control protocol, pessimistic or optimistic
    - serializable schedule
        - equivalent to some serial execution (the "right" execution order)
    - conflicts
        - read-write conflict 即 unrepeatable read
        - write-read conflict 即 dirty read
        - write-write conflict 即 lost update

---

- isolation level
    - serializable
    - repeatable reads
    - read commit
    - read uncommitted

单靠 2PL 解决不了 DML 造成的数据增加、减少，也就是 phantom read 的问题
需要更强的隔离级别，也就是 serializable
比 2PL 更强，需要加上 index lock，确保其他 DML 修改字段不影响当前事务


---

### two-phase locking

- lock types
    - S-LOCK, shared lock for reads
    - X-LOCK, exclusive locks for writes

- 2PL
    - pessimistic
    - growing + shrinking
        - 为什么 shrinking 阶段不允许获取新的锁呢
    - cascading abort
        - 假设 txn_1 和 txn_2 都读取了字段 A，可能出现 dirty read
        - 因为没有 snapshot，txn_1 释放 A 的锁，txn_2 就能读到 txn_1 写入的值（此时 txn_1 还没结束
        - 如果 txn_1 abort 了，txn_2 也必须 abort，否则数据就有问题
        - 如果还有 txn_3 受到 txn_2 影响，也要 abort
    - dirty read 可以用 strict 2PL 解决
        - 比较粗暴，txn 完成前都不释放锁，就不会被其他事务读取到了
    - deadlock 可以靠 detection 或 prevention 解决

- deadlock detection
    - waits-for graph
    - 有向图，节点是 transaction，txn_2->txn_1 表示 txn_2 在等待 txn_1 释放锁
    - 定期处理 waits-for graph，发现循环则处理
- deadlock prevention
    - 每个 transaction 都有一个 priority，可以是 timestamp（越早的事务优先级越高）
    - wait-die (old waits for young)
        - 如果高优先级的事务需要等待一个锁，那就等着。如果低优先级的事务需要等待一个锁，直接 abort。
    - would-wait (young waits for old)
        - 如果高优先级的事务需要等待一个锁，那么持有锁的事务 abort。如果低优先级的事务等待一个锁，那就等着。
        - 也就是优先完成先开始的 transaction
    - abort 之后重新开始，priority 不变。（不然，可能事务一直低优先级

- intention lock
    - 如果要修改很多字段，会导致 buffer 需要获取大量 latch
    - intention lock 是说，在 high level 获取 lock 之后，子节点就不需要检查 lock 了
    - 会多出一些新的锁类型 Intention-Shared / Intention-Exclusive / Shared+Intention-Exclusive

---

### timestamp ordering

- basic T/O
    - timestamp
        - 每一个事务都携带 timestamp-txn
        - 每一个数据都携带 timestamp-read 和 timestamp-write
    - read
        - 如果 txn 发现 timestamp-write(X) > timestamp-txn，说明 X 被其他事务修改过，abort 重来
        - 读取后要看下是否需要更新 timestamp-read(X)（如果 timestamp-read(X) > timestamp-txn，就不需要了
    - write
        - 如果 txn 发现 timestamp-write(X) > timestamp-txn，说明 X 被其他事务修改过，abort 重来
            - 一个优化（thomas write rule），此时不 abort 而是忽略 write，假装已经写入 X，之后又被新事务覆盖了
        - 如果 txn 发现 timestamp-read(X) > timestamp-txn，说明其他事务在用 X，abort 重来
        - 写入后要更新 timestamp-write(X)
    - 不管 read 还是 write，完成后都会 copy 一个 X，保证后续的 repeatable read
    - abort 重启后，就是新的 timestamp-txn
    - read/write 时的 timestamp 比较，保证了 txn read/write 都是自己之前的事务的结果
        - 换言之，事务之间不会出现交叉修改
    - 不可能 deadlock，毕竟都直接 abort 了，根本没有 lock
        - 和 2PL 时进行 deadlock prevention 起到了一样的效果，不死锁
    - 一个问题是，如果事务耗时较长，可能一直被小事务冲突，导致无法完成
    - 怎么避免 dirty read？
        - 其他事务，开始比我早，结束比我晚，它 abort 了，我怎么办
        - 需要保证 recoverable schedule，即我读到的 txn，都已经 commit 了
        - strict 2PL 靠锁保证没有 dirty read，后面的事务都先等着
    - basic TO 没有被实际应用……

- OCC (optimistic concurrency control
    - 三步走
        - read (track read/write set + write in a private workspace) （有 snapshot 的意思了
        - validate (before commit)
        - write (commit or abort)
    - 不需要 timestamp-read(X) 了
    - 在 validate 阶段，才去生成 timestamp-txn
    - validate 阶段，要保证 read 阶段读写的值，都是最新的
        - 假设 txn_1 写入了 X，正在 validate。如果 txn_2 还没 validate，且读取了 X，那么 txn_1 需要 abort。初看有点奇怪，为什么不是 txn_2 abort。
        - 保证
            - writeSet(txn_1) 与 readSet(txn_2) 的交集为空
            - writeSet(txn_1) 与 writeSet(txn_2) 的交集为空
    - OCC 的不足
        - 把读写的数据 copy 到本地，开销有点大
        - abort 的场景，浪费的计算资源比 2PL 要大一点。2PL 是完成部分事务后等待，OCC 是已经完成全部事务
            - 冲突太多的情况，2PL 和 OCC 的效率其实也差不了多少，2PL 也要等待锁
        - validate 阶段对比 writeSet/readSet，是需要加 latch 的
    - OCC 更适合读多写少的场景
        - 毕竟写少就冲突就少
        - 不过写少的话都是 S-LOCK，加锁也可能开销也不大？

- partition T/O
    - 在 sharding 的场景下，如果只涉及一个 partition，可以在该节点内完成 OCC

---

### MVCC

- multiple physical versions of a single logical object
- readers don't block writers, writers don't block readers

- MVCC 和之前说的 pessimistic/optimistic concurrency control 是不同的概念
    - MVCC 只是一种 snapshot 的实现方案
    - 最终是否提交某个 transaction，要实现哪种 isolation，不是 MVCC 能决定的

- MVCC design
    - concurrency control
        - 使用 2PL 或 OCC 来判断，是否允许 write
    - version
        - multiple-version 的 version 存储在哪
        - 讲的三种方法，我没感觉到有什么很大区别……
        - (version, value, pointer_to_old_value_list) 基本都是这样的啊
    - garbage collection
        - 什么样算是不再使用，不再使用了什么时候回收
        - tuple-level or transaction-level
        - transaction-level，transaction 知道自己生成了哪些版本，结束后清理掉
        - tuple-level，需要检查
            - background vacuuming, mark-and-sweep
            - cooperative cleaning (only work for O2N 遍历的时候发现 old 没用了就删掉
        - RC vs tracing 呀
    - index

---

## recovery

- crash recovery algorithm, has two parts
    - at runtime, do something to ensure that the DBMS can recover from a failure
    - after failure, do something to recover the database (use the infomations collected from first step

### buffer

- txn_1 修改了字段 A，txn_2 修改了字段 B，AB 恰好在一个 page 里
- 当 txn_1 commit 的时候，txn_2 还在处理中，这时要不要把 page 写入 disk
- 如果写入了，之后 txn_2 abort 怎么办

- STEAL policy 是否允许，将 txn_2 还没 commit 的 B 写回 disk（txn_1 steal page from txn_2
    - STEAL 允许
    - NO-STEAL 不允许
- FORCE policy 要不要在 commit 之前，先将修改写入 disk
    - FORCE 必须先写入 disk
    - NO-FORCE 不需要

- shadow paging (NO-STEAL + FORCE) (better recovery performance)
    - txn_1 commit 时，force 要求先将 A 写入 disk，no-steal 要求不能将 B 写入 disk
    - 修改 B 前将 page copy 一份，这样 page 就只含有对 B 的修改，可以写入 disk
        - 和 MVCC 的一个区别，MVCC 是 tuple 级别的，这是 page 级别的
    - txn_2 也是自己的一份 copy，abort 之后删除 buffer 中的 page 即可
    - crash 之后恢复，读取 disk 即可

- write-ahead logging (STEAL + NO-FORCE) (better runtime performance)
    - 允许 steal 和 no-force 的前提是写入了 write-ahead log
    - log 包含了 txn_id, key, new_val, old_val（方便 undo/redo
    - force 的对象从 page 变成了 log
    - crash 之后会从 log 恢复，所以 steal 造成的脏数据不再是问题
    - logging schema
        - physiological (the most popular) vs logical vs physical
        - 记录每个 tuple 的变更（同 physical），但是不记录 tuple 具体的 offset，而是记录 tuple 的 ID
    - LSN (log sequence number)
        - sequence number 就是个逻辑时钟吧
        - pageLSN_x 是 page_x 最后一次被修改的时间
        - flushedLSN 是 WAL 被写入 disk 的时间
        - 在 page_x 被写入 disk 之前，要保证 flushedLSN >= pageLSN_x。也就是 page 对应的 log 都已经写入 disk 了
            - log 不需要等待任何其他操作，可以不断 flush 到 disk（可以 batch 降低写入的开销
            - page 需要等待 log 被 flush，之后才能从 buffer pool 里剔除
            - txn 不需要等待 page，也是等待 log，不会受 page flush 时间的影响
    - txn abort
        - log 是 append only 的，所以 abort 也是在追加操作
        - txn 里的每个 log，都加上一个 prevLSN，方便 abort 时找到要消除的操作
    - checkpoint
        - 什么时候更新 checkpoint
        - 比较 naive 的方式是在更新 checkpoint 时，等待当前事务结束，暂停新事务开启
        - 肯定不能接受，如何改进
        - fuzzy checkpoint
            - 把 checkpoint 变成 checkpoint-begin / checkpoint-end 两个事件

### ARIES

- analysis 找到 checkpoint
- redo 根据 log 修改 page
- undo 消除 abort 的 txn 带来的修改

---

## distributed

distributed 的内容，看 MIT 课程吧

- atomic commit protocol
    - 2PC
    - raft
- replication and partition
- consistency (CAP)
- execute model
    - push query to data
        - 存储和计算在一起，请求会被分发到不同节点再聚合起来，聚合时只传输了过滤后的数据
        - 扩展的时候，计算和存储是一起扩展的
    - pull data to query
        - 计算节点收到请求后，去存储节点拉数据（这个过程传输了全部数据）之后再聚合计算结果
        - 这样更方便对存储和计算进行分离？
        - 计算可以横向扩展，存储也也可以横向扩展，两者的扩展互不影响
    - 有一点是一样的，计算都会被分配到多个节点
        - 因为分布式数据库一个需求来源就是单机装不下完整数据
        - 分配到哪个节点，是 sharding 的方式决定的
    - 计算节点在计算过程中宕机，如何恢复
        - 看计算类型，如果是 OLTP，大概重来？
        - 如果是 OLAP，那么像 spark 那样部分步骤持久化，重新计算没持久化的步骤
- distributed query plan
    - the location of data in the cluster and data movement costs（优化的难度加大了
    - 方案一，生成 query plan，再根据 sharding 拆分
    - 方案二，根据 sharding 情况，为每个节点生成 query plan（可以根据节点情况进行特定优化
- distributed join
    - 可能需要传输很多数据
    - 考虑 semi-join （即 projection pushdown，只传输会使用到的字段
