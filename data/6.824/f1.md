# F1

- introduction
    - F1 is a fault-tolerant globally-distributed OLTP and OLAP database
        - scalability, scale up by adding resources
        - availability, never go down for any reason
        - consistency, ACID transactions
        - usability, full SQL query support
    - F1 is built on top of Spanner
        - Spanner
            - scalable data storage
            - synchronous replication
            - strong consistency
            - ordering properties
    - design choice (synchronous replication) in F1 result in higher latency for typical reads and writes
        - mitigate latency by using a hierarchical schema model with structured data types
            - use a coarse schema design with rich column types
        - and smart application design ğŸ˜‚

- architecture
    - F1 client -- load balancer -- N*(F1 server -- Spanner -- Colossus)
        - F1 servers, F1 master, slave pool
        - master å°±ä¸€ä¸ªã€‚å¯ä»¥é€šè¿‡å¢åŠ  server,slave,spanner æé«˜ååé‡
        - ç”¨æˆ·é€šè¿‡è´Ÿè½½å‡è¡¡ï¼Œä¸ F1 server é€šä¿¡
        - F1 master ä¸»è¦æ˜¯ç®¡ç† slave çš„ membershipï¼Œå…·ä½“å·¥ä½œæ˜¯ slave åœ¨æ‰§è¡Œ
    - F1 servers are mostly stateless (é™¤é client è¦æ±‚ä½¿ç”¨æ‚²è§‚é”å®ç°äº‹åŠ¡)
    - F1 servers are typically co-located in the same set of datacenters as the Spanner servers storing the data
        - F1 server can communicate with Spanner servers outside their own datacenter when necessary
    - the commit latency is relatively high (50-150ms)

- data model
    - hierarchical schema
        - tables in the F1 schema can be organized into a hierarchy
        - it reduces the number of Spanner groups involved in a transaction
    - protocol buffer
        - the F1 data model supports table columns that contain structured data types (PB)
        - many tables in an F1 schema consist of just a single PB column
        - the entire PB is effectively treated as one blob by Spanner
        - PB allow the use of repeated fields
        - ï¼ˆå’Œä½¿ç”¨ JSON çš„åŸå› å·®ä¸å¤šå‘€
    - indexing
        - indexes are stored as separate table in Spanner

- schema changes
    - all schema changes are fully non-blocking
    - prevent two F1 servers update the database concurrently using different schemas
    - enforcing that across all F1 servers, at most two different schemas are active
        - grant leases on the schema
        - no server uses a schema after lease expiry

- transactions
    - consistency problems should be solved at the database level
    - F1 implements three types of transactions, built on top of Spanner's transaction support
        - snapshot transactions
        - pessimistic transaction
        - optimistic transaction (F1 clients use this by default)
            - read phase ä¸åŠ é”
                - F1 returns with each row its last modification timestamp
            - write phase
                - F1 client collects these timestamps and passes them back to an F1 server
                - F1 server creates a short-lived Spanner pessimistic transaction
                    - re-read the last modification timestamps for all read rows
    - optimistic transaction drawbacks
        - do not prevent insertion phantoms
        - low throughput under high contention

- query processing
    - TODO

- deployment
    - read-only replicas are used only for snapshot reads
        - thus allow us to segregate OLTP and OLAP workloads
    - 3-way replication is not enough, use 5-way Paxos replication to ensure high availability
        - F1 åœ¨ç¾å›½éƒ¨ç½²äº†äº”ä¸ªèŠ‚ç‚¹ï¼Œä¸œè¥¿æµ·å²¸å„ä¸¤ä¸ªï¼Œä¸­éƒ¨ä¸€ä¸ª
        - ï¼ˆcommit æœ€å° 50ms çš„å»¶è¿Ÿ
    - transaction latency is best when clients and F1 servers are co-located with Spanner leader replicas
        - designate one of the datacenters as a preferred leader location

- latency and throughput
    - Paxos è¦æ±‚ major ä¸€è‡´ï¼Œå†åŠ ä¸Š 2PC ç¡®è®¤ï¼Œå¯¼è‡´äº†é«˜å»¶è¿Ÿ (50-150ms)
        - ç”¨æˆ·å“åº”å¹³å‡åœ¨ 200msï¼ˆä¸çŸ¥é“è¿™é‡Œçš„å¹³å‡æ˜¯æŒ‡å“ªä¸ªå€¼
        - avoiding serial reads in client code accounts for much of that
    - resource costs are higher in F1
        - queries often use an order of magnitude more CPU than similar MySQL queries
    - for non-interactive applications that apply bulk updates, we optimize for throughput rather than latency
    - for query processing, we have focused on functionality and partity, and noy on absolute query performance
